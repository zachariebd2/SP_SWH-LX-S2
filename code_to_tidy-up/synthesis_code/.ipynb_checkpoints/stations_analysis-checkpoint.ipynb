{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "998ba42e-afc8-4990-abe5-7a22e9919054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import errno\n",
    "import re\n",
    "import glob\n",
    "import csv\n",
    "import os.path as op\n",
    "import json\n",
    "import math\n",
    "from datetime import datetime, timedelta, date\n",
    "from IPython.display import display, clear_output, HTML\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from osgeo import osr, ogr, gdal\n",
    "from osgeo.gdalnumeric import *\n",
    "from osgeo.gdalconst import *\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cbook as cbook\n",
    "import matplotlib.gridspec as pltg\n",
    "import seaborn as sn\n",
    "from matplotlib.colors import LogNorm\n",
    "import calendar\n",
    "import pyproj\n",
    "from pyproj import Proj, transform\n",
    "import warnings\n",
    "import rasterio\n",
    "import pickle\n",
    "from rasterio.warp import calculate_default_transform , reproject, Resampling \n",
    "from rasterio.mask import mask\n",
    "from rasterio.merge import merge\n",
    "from shapely.geometry import mapping\n",
    "import scipy.stats as st\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import geopandas as gpd\n",
    "from calendar import isleap\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "943b7def-e24b-4e01-974b-bfafb06dfc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_p(dos):\n",
    "    try:\n",
    "        os.makedirs(dos)\n",
    "    except OSError as exc:  # Python >2.5\n",
    "        if exc.errno == errno.EEXIST and os.path.isdir(dos):\n",
    "            pass\n",
    "        else:\n",
    "            raise\n",
    "            \n",
    "def getDateFromStr(N):\n",
    "    sepList = [\"\",\"-\",\"_\",\"/\"]\n",
    "    date = ''\n",
    "    for s in sepList :\n",
    "        found = re.search('\\d{4}'+ s +'\\d{2}'+ s +'\\d{2}', N)\n",
    "        if found != None :\n",
    "           date = datetime.strptime(found.group(0), '%Y'+ s +'%m'+ s +'%d').date()\n",
    "           break\n",
    "    return date\n",
    "    \n",
    "def getTileFromStr(N):\n",
    "\n",
    "    tile = ''\n",
    "    found = re.search('\\d{2}' +'[A-Z]{3}', N)\n",
    "    if found != None : tile = found.group(0)\n",
    "    return tile\n",
    "# reproject the coord of a point from inEPSG to outEPSG\n",
    "def reproject(inEPSG,outEPSG,x1,y1):  \n",
    "    #transformer = Transformer.from_crs(int(inEPSG),int(outEPSG))\n",
    "    #x2,y2 = transformer.transform(y1, x1)\n",
    "    inProj = Proj(init='EPSG:' + inEPSG)\n",
    "    outProj = Proj(init='EPSG:'+ outEPSG)\n",
    "    x2,y2 = transform(inProj,outProj,x1,y1)\n",
    "    #print(\"REPROJECT\")\n",
    "    #print(inEPSG,x1,y1)\n",
    "    #print(outEPSG,x2,y2)\n",
    "    return [x2, y2]\n",
    "\n",
    "def getSPOTGRSFromStr(N):\n",
    "    K = ''\n",
    "    J = ''\n",
    "    found = re.search('_'+'\\d{3}'+'-' +'\\d{3}'+'-'+'\\d{1}', N)\n",
    "    if found != None : \n",
    "        GRS = found.group(0)\n",
    "        GRS_s = GRS.split('-')\n",
    "        K = int(GRS_s[0][1:])\n",
    "        J = int(GRS_s[1])\n",
    "        S = int(GRS_s[2])\n",
    "    \n",
    "    return K,J,S\n",
    "\n",
    "def getCoords(G):\n",
    "    \n",
    "    \n",
    "    GT = G.GetGeoTransform()\n",
    "    minx = GT[0]\n",
    "    maxy = GT[3]\n",
    "    maxx = minx + GT[1] * G.RasterXSize\n",
    "    miny = maxy + GT[5] * G.RasterYSize\n",
    "    \n",
    "    return minx, maxy, maxx, miny\n",
    "\n",
    "def get_station_data(station_id, min_year = 1985, max_year = 2019): \n",
    "    '''\n",
    "    Returns a dataframe of the snow depth time series at this station\n",
    "    '''\n",
    "    f = glob(f'data/output_*_{int(station_id)}.txt')[0]\n",
    "    df = pd.read_csv(f, delimiter=';', parse_dates=['Date'])\n",
    "    df.rename(columns={\"Date\": \"date\"}, inplace=True)\n",
    "    df.set_index('date',inplace=True)\n",
    "    df = df.loc[f'{min_year}-09-01':f'{max_year}-08-31']\n",
    "    df = add_waterdate(df)\n",
    "    return df\n",
    "\n",
    "def add_waterdate(df):\n",
    "    '''\n",
    "    Appends a water year column ('wy') to a series dataframe (index = date object) and other useful dates-related columns ('doy', 'dowy', 'datewy', etc.)\n",
    "    '''\n",
    "    dt = df.index\n",
    "    df['date'] = dt\n",
    "    df['doy'] = dt.dayofyear #day_of_year\n",
    "    df['year'] = dt.year\n",
    "    # add water year (labeled with 1st year of water year)\n",
    "    df['wy'] = df[['doy','year']].apply(axis=1, func = lambda x: (x[1] - (x[0] < 244 + isleap(x[1]))), raw=True)\n",
    "    df['dowy'] = df[['doy','year','wy']].apply(axis=1, func = lambda x: (x[0] + 122*(x[1] > x[2]) - (243+isleap(x[1]))*(x[1] == x[2])  ), raw=True) #day_of_wateryear\n",
    "    return df.drop('date',axis=1)\n",
    "\n",
    "def daterange(start_date, end_date):\n",
    "    for n in range(int((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ecc281fe-6f87-45d6-837e-4fe3b4b2129d",
   "metadata": {},
   "outputs": [],
   "source": [
    "S2_tiles={\n",
    "    \"PYR\":\n",
    "    {\n",
    "        \"30TXN\":{'EPSG':'32630','MINX':600000,'MINY':4690200,'MAXX':709800,'MAXY':4800000},\n",
    "        '30TYN':{'EPSG':'32630','MINX':699960,'MINY':4690200,'MAXX':809760,'MAXY':4800000},\n",
    "        '31TCH':{'EPSG':'32631','MINX':300000,'MINY':4690200,'MAXX':409800,'MAXY':4800000},\n",
    "        '31TDH':{'EPSG':'32631','MINX':399960,'MINY':4690200,'MAXX':509760,'MAXY':4800000}\n",
    "    },\n",
    "    \"ALP\":\n",
    "    {\n",
    "        \"31TGJ\":{'EPSG':'32631','MINX':699960,'MINY':4790220,'MAXX':809760,'MAXY':4900020},\n",
    "        '31TGK':{'EPSG':'32631','MINX':699960,'MINY':4890240,'MAXX':809760,'MAXY':5000040},\n",
    "        '31TGL':{'EPSG':'32631','MINX':699960,'MINY':4990200,'MAXX':809760,'MAXY':5100000},\n",
    "        '31TGM':{'EPSG':'32631','MINX':699960,'MINY':5090220,'MAXX':809760,'MAXY':5200020},\n",
    "        \"32TLP\":{'EPSG':'32632','MINX':300000,'MINY':4790220,'MAXX':409800,'MAXY':4900020},\n",
    "        '32TLQ':{'EPSG':'32632','MINX':300000,'MINY':4890240,'MAXX':409800,'MAXY':5000040},\n",
    "        '32TLR':{'EPSG':'32632','MINX':300000,'MINY':4990200,'MAXX':409800,'MAXY':5100000},\n",
    "        '32TLS':{'EPSG':'32632','MINX':300000,'MINY':5090220,'MAXX':409800,'MAXY':5200020}\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "epsg_list={\n",
    "    \"30TXN\":{'EPSG':'32630'},\n",
    "    '30TYN':{'EPSG':'32630'},\n",
    "    '31TCH':{'EPSG':'32631'},\n",
    "    '31TDH':{'EPSG':'32631'},\n",
    "    \"31TGJ\":{'EPSG':'32631'},\n",
    "    '31TGK':{'EPSG':'32631'},\n",
    "    '31TGL':{'EPSG':'32631'},\n",
    "    '31TGM':{'EPSG':'32631'},\n",
    "    \"32TLP\":{'EPSG':'32632'},\n",
    "    '32TLQ':{'EPSG':'32632'},\n",
    "    '32TLR':{'EPSG':'32632'},\n",
    "    '32TLS':{'EPSG':'32632'}\n",
    "}\n",
    "\n",
    "\n",
    "S2_4326_tiles={\n",
    "    \"PYR\":\n",
    "    {\n",
    "        \"30TXN\":{'EPSG':'32630','MINX':reproject('32630','4326',600000,4690200)[0],'MINY':reproject('32630','4326',600000,4690200)[1],'MAXX':reproject('32630','4326',709800,4800000)[0],'MAXY':reproject('32630','4326',709800,4800000)[1]\n",
    "                },\n",
    "        '30TYN':{'EPSG':'32630','MINX':reproject('32630','4326',699960,4690200)[0],'MINY':reproject('32630','4326',699960,4690200)[1],'MAXX':reproject('32630','4326',809760,4800000)[0],'MAXY':reproject('32630','4326',809760,4800000)[1]\n",
    "                },\n",
    "        '31TCH':{'EPSG':'32631','MINX':reproject('32631','4326',300000,4690200)[0],'MINY':reproject('32631','4326',300000,4690200)[1],'MAXX':reproject('32631','4326',409800,4800000)[0],'MAXY':reproject('32631','4326',409800,4800000)[1]\n",
    "                },\n",
    "        '31TDH':{'EPSG':'32631','MINX':reproject('32631','4326',399960,4690200)[0],'MINY':reproject('32631','4326',399960,4690200)[1],'MAXX':reproject('32631','4326',509760,4800000)[0],'MAXY':reproject('32631','4326',509760,4800000)[1]\n",
    "                }\n",
    "    },\n",
    "    \"ALP\":\n",
    "    {\n",
    "        \"31TGJ\":{'EPSG':'32631','MINX':reproject('32631','4326',699960,4790220)[0],'MINY':reproject('32631','4326',699960,4790220)[1],'MAXX':reproject('32631','4326',809760,4900020)[0],'MAXY':reproject('32631','4326',809760,4900020)[1]\n",
    "                },\n",
    "        '31TGK':{'EPSG':'32631','MINX':reproject('32631','4326',699960,4890240)[0],'MINY':reproject('32631','4326',699960,4890240)[1],'MAXX':reproject('32631','4326',809760,5000040)[0],'MAXY':reproject('32631','4326',809760,5000040)[1]\n",
    "                },\n",
    "        '31TGL':{'EPSG':'32631','MINX':reproject('32631','4326',699960,4990200)[0],'MINY':reproject('32631','4326',699960,4990200)[1],'MAXX':reproject('32631','4326',809760,5100000)[0],'MAXY':reproject('32631','4326',809760,5100000)[1]\n",
    "                },\n",
    "        '31TGM':{'EPSG':'32631','MINX':reproject('32631','4326',699960,5090220)[0],'MINY':reproject('32631','4326',699960,5090220)[1],'MAXX':reproject('32631','4326',809760,5200020)[0],'MAXY':reproject('32631','4326',809760,5200020)[1]\n",
    "                },\n",
    "        \"32TLP\":{'EPSG':'32632','MINX':reproject('32632','4326',300000,4790220)[0],'MINY':reproject('32632','4326',300000,4790220)[1],'MAXX':reproject('32632','4326',409800,4900020)[0],'MAXY':reproject('32632','4326',409800,4900020)[1]\n",
    "                },\n",
    "        '32TLQ':{'EPSG':'32632','MINX':reproject('32632','4326',300000,4890240)[0],'MINY':reproject('32632','4326',300000,4890240)[1],'MAXX':reproject('32632','4326',409800,5000040)[0],'MAXY':reproject('32632','4326',409800,5000040)[1]\n",
    "                },\n",
    "        '32TLR':{'EPSG':'32632','MINX':reproject('32632','4326',300000,4990200)[0],'MINY':reproject('32632','4326',300000,4990200)[1],'MAXX':reproject('32632','4326',409800,5100000)[0],'MAXY':reproject('32632','4326',409800,5100000)[1]\n",
    "                },\n",
    "        '32TLS':{'EPSG':'32632','MINX':reproject('32632','4326',300000,5090220)[0],'MINY':reproject('32632','4326',300000,5090220)[1],'MAXX':reproject('32632','4326',409800,5200020)[0],'MAXY':reproject('32632','4326',409800,5200020)[1]\n",
    "                }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "LANDSAT_tiles_2={\n",
    "    \"ALP\":\n",
    "    {\n",
    "        \"31TGJ\":[\"195029\",\"196029\"],\n",
    "        '31TGK':[\"195029\",\"196029\",\"196028\"],\n",
    "        '31TGL':[\"195029\",\"195028\",\"196029\",\"196028\"],\n",
    "        '31TGM':[\"195028\",\"196028\"],\n",
    "        \"32TLP\":[\"194029\",\"195029\"],\n",
    "        '32TLQ':[\"194029\",\"195029\",\"195028\",\"196029\",\"196028\"],\n",
    "        '32TLR':[\"194029\",\"195029\",\"195028\",\"196029\",\"196028\"],\n",
    "        '32TLS':[\"195028\",\"196028\"]\n",
    "    },\n",
    "    \"PYR\":\n",
    "    {\n",
    "        \"30TXN\":[\"200030\",\"199030\"],\n",
    "        '30TYN':[\"200030\",\"199030\"],\n",
    "        '31TCH':[\"199030\",\"198031\",\"198030\"],\n",
    "        '31TDH':[\"198031\",\"198030\",\"197031\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "LANDSAT_tiles={\n",
    "    \"ALP\":\n",
    "    {\n",
    "        \"195029\":[\"31TGJ\",'31TGK','31TGL',\"32TLP\",'32TLQ','32TLR'],\n",
    "        \"195028\":['31TGL','31TGM','32TLQ','32TLR','32TLS'],\n",
    "        \"196029\":[\"31TGJ\",'31TGK','31TGL','32TLQ','32TLR'],\n",
    "        \"196028\":['31TGK','31TGL','31TGM','32TLQ','32TLR','32TLS'],\n",
    "        \"194029\":[\"32TLP\",'32TLQ','32TLR']\n",
    "    },\n",
    "    \"PYR\":\n",
    "    {\n",
    "        \"200030\":[\"30TXN\",'30TYN'],\n",
    "        \"199030\":[\"30TXN\",'30TYN','31TCH'],\n",
    "        \"198031\":['31TCH','31TDH'],\n",
    "        \"198030\":['31TCH','31TDH'],\n",
    "        \"197031\":['31TDH']\n",
    "    }\n",
    "}\n",
    "\n",
    "SPOT_tile={\n",
    "    \"ALP\":\n",
    "    {\"KMIN\":46,\n",
    "     \"KMAX\":55,\n",
    "     \"JMIN\":254,\n",
    "     \"JMAX\":263\n",
    "    },\n",
    "    \"PYR\":\n",
    "    {\"KMIN\":35,\n",
    "     \"KMAX\":48,\n",
    "     \"JMIN\":262,\n",
    "     \"JMAX\":265\n",
    "    }\n",
    "}\n",
    "def get_station_data(station_id, min_year = 1985, max_year = 2019): \n",
    "    '''\n",
    "    Returns a dataframe of the snow depth time series at this station\n",
    "    '''\n",
    "    f = glob.glob(f'/home/ad/barrouz/zacharie/TIMESERIES_PROJECT/INSITU/STATIONS/data/output_*_{int(station_id)}.txt')[0]\n",
    "    df = pd.read_csv(f, delimiter=';', parse_dates=['Date'])\n",
    "    df.rename(columns={\"Date\": \"date\"}, inplace=True)\n",
    "    df.set_index('date',inplace=True)\n",
    "    df = df.loc[f'{min_year}-09-01':f'{max_year}-08-31']\n",
    "    df = add_waterdate(df)\n",
    "    return df\n",
    "\n",
    "def add_waterdate(df):\n",
    "    '''\n",
    "    Appends a water year column ('wy') to a series dataframe (index = date object) and other useful dates-related columns ('doy', 'dowy', 'datewy', etc.)\n",
    "    '''\n",
    "    dt = df.index\n",
    "    df['date'] = dt\n",
    "    df['doy'] = dt.dayofyear #day_of_year\n",
    "    df['year'] = dt.year\n",
    "    # add water year (labeled with 1st year of water year)\n",
    "    df['wy'] = df[['doy','year']].apply(axis=1, func = lambda x: (x[1] - (x[0] < 244 + isleap(x[1]))), raw=True)\n",
    "    df['dowy'] = df[['doy','year','wy']].apply(axis=1, func = lambda x: (x[0] + 122*(x[1] > x[2]) - (243+isleap(x[1]))*(x[1] == x[2])  ), raw=True) #day_of_wateryear\n",
    "    return df.drop('date',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cbc3da40-ad7d-40ff-b974-45dce7ebc960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "477\n"
     ]
    }
   ],
   "source": [
    "\n",
    "points_shp_path = \"/home/ad/barrouz/zacharie/TIMESERIES_PROJECT/INSITU/STATIONS/stations_tcd_50_massifs.shp\"\n",
    "srs =  osr.SpatialReference()\n",
    "srs.ImportFromEPSG(4326)\n",
    "drv = ogr.GetDriverByName( 'ESRI Shapefile' )\n",
    "points_shp = drv.Open(points_shp_path)\n",
    "\n",
    "layer_points = points_shp.GetLayer()\n",
    "featureCount = layer_points.GetFeatureCount()\n",
    "print(featureCount)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02700f33-900c-410e-8109-b5eb291568ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d21c379a-ce09-45fd-8f24-bc9a67fe1dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parse stations 99%                                               \r"
     ]
    }
   ],
   "source": [
    "dict_tile_stations = {}\n",
    "\n",
    "for f,feature in enumerate(layer_points):\n",
    "    lon = feature['lon']\n",
    "    lat = feature['lat']\n",
    "    mtn = feature['mtn']\n",
    "    print(f\"parse stations {int(f/len(layer_points)*100)}%\",end=\"                                               \\r\")\n",
    "    for tile in S2_4326_tiles[mtn]:\n",
    "        if lon > S2_4326_tiles[mtn][tile][\"MAXX\"] or lon < S2_4326_tiles[mtn][tile][\"MINX\"] or \\\n",
    "        lat > S2_4326_tiles[mtn][tile][\"MAXY\"] or lat < S2_4326_tiles[mtn][tile][\"MINY\"] : continue\n",
    "        if tile not in dict_tile_stations : dict_tile_stations[tile] = []\n",
    "        to_tile = pyproj.Transformer.from_crs(4326,int(epsg_list[tile]['EPSG']), always_xy=True)\n",
    "        coord = to_tile.itransform([(lon,lat)]) \n",
    "        list_coord = [*coord] #only one coord\n",
    "        dict_tile_stations[tile].append((feature,list_coord,get_station_data(name, min_year = 1985, max_year = 2015)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2961d735-dcfa-415f-9103-9b6a91ddfb27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SWH_path = \"/home/ad/barrouz/zacharie/TIMESERIES_PROJECT/SWH_INFERENCE/TCD-BLUE_AVG-1200\"\n",
    "list_swh_dates = list(dict.fromkeys(\n",
    "[date(int(x.split(\"/\")[-3]),int(x.split(\"/\")[-2]),int(x.split(\"/\")[-1])) for x in glob.glob(os.path.join(SWH_path,'*','*','*','*','*'))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "23346d9c-671e-4abb-9218-fd6c7f19cdf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DLR_path = \"/home/ad/barrouz/zacharie/TIMESERIES_PROJECT/LANDSAT_QA_DLR\"\n",
    "list_dlr_dates = list(dict.fromkeys(\n",
    "[getDateFromStr(x) for x in glob.glob(os.path.join(DLR_path,'*','*'))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2db177d9-1163-4492-9088-c8303628deb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALP 32TLS 99%  393133                                               \r"
     ]
    }
   ],
   "source": [
    "#get days of spot acqui per month/year/station...\n",
    "\n",
    "SWH_path = \"/home/ad/barrouz/zacharie/TIMESERIES_PROJECT/SWH_INFERENCE/TCD-BLUE_AVG-1200\"\n",
    "df_swh_path = \"/home/ad/barrouz/zacharie/TIMESERIES_PROJECT/INSITU/STATIONS/df_swh_acquis_stations.pkl\"\n",
    "dict_swh_path = \"/home/ad/barrouz/zacharie/TIMESERIES_PROJECT/INSITU/STATIONS/dict_swh_acquis_stations.json\"\n",
    "\n",
    "\n",
    "d = dict()\n",
    "d[\"ID\"] = []\n",
    "d[\"YEAR\"] = []\n",
    "d[\"MONTH\"] = []\n",
    "d[\"SEASON\"] = []\n",
    "d[\"DAY\"] = []\n",
    "d[\"MTN\"] = []\n",
    "d[\"TILE\"] = []\n",
    "d[\"SAFRAN\"] = []\n",
    "d[\"SAFRAN_S\"] = []\n",
    "d[\"CLASS\"] = []\n",
    "d[\"SD\"] = []\n",
    "nb_dates = len(list_swh_dates)\n",
    "count = 0\n",
    "for mtn in S2_4326_tiles:\n",
    "    for tile in S2_4326_tiles[mtn]:\n",
    "        for j,single_date in enumerate(list_swh_dates):\n",
    "            year = single_date.year\n",
    "            month = single_date.month\n",
    "            day = single_date.day\n",
    "            #get list of swh products\n",
    "            print(f\"{mtn} {tile} {int(j/nb_dates*100)}%  {count}\",end=\"                                               \\r\")\n",
    "            list_swh = glob.glob(os.path.join(SWH_path,mtn,'*',str(year),str(month),str(day),\"*\",tile,\"*\",\"*FSCTOC*\"),recursive=True)\n",
    "            if not list_swh: continue\n",
    "            #get list of stations with data at that date for that tile\n",
    "            list_stations = []\n",
    "            v = 0\n",
    "            for feature,list_coord,station_df in dict_tile_stations[tile]:\n",
    "                v+=1\n",
    "                #print(f\"find stations {mtn} {tile} {day}/{month}/{year} {int(v/len(dict_tile_stations[tile])*100)}% {count}\",end=\"                                               \\r\")\n",
    "                try:\n",
    "                    sd = station_df.loc[single_date.strftime(\"%Y-%m-%d\"), 'DSN_T_ISBA_Crocus_assim']\n",
    "                    list_stations.append((feature,list_coord,sd))\n",
    "                except KeyError:\n",
    "                    continue\n",
    "            if not list_stations : continue\n",
    "            #get hydro date\n",
    "            if single_date < date(int(single_date.year),9,1):\n",
    "                annee_hydro = int(single_date.year) - 1\n",
    "            else:\n",
    "                annee_hydro = int(single_date.year)\n",
    "            date_debut_hydro = date(annee_hydro,9,1)\n",
    "            annee_hydro_jour = (single_date - date_debut_hydro).days + 1\n",
    "            #check for swh/stations pairs\n",
    "            for k,swh in enumerate(list_swh):\n",
    "                #show progress\n",
    "                #print(f\"find pairs {mtn} {tile} {day}/{month}/{year} {int(k/len(list_swh)*100)}% {count}\",end=\"                                               \\r\")\n",
    "                with rasterio.open(swh) as src:\n",
    "                    for feature,list_coord,sd in list_stations:\n",
    "                        for z in src.sample(list_coord,masked=True): \n",
    "                            if z[0] != '--' : \n",
    "                                pixel = float(z[0])\n",
    "                                #print(pixel)\n",
    "                                if pixel != 255 :\n",
    "                                    d[\"YEAR\"].append(annee_hydro)\n",
    "                                    d[\"DAY\"].append(annee_hydro_jour)\n",
    "                                    d[\"MONTH\"].append(month)\n",
    "                                    d[\"SEASON\"].append(season)\n",
    "                                    d[\"TILE\"].append(tile)\n",
    "                                    d[\"MTN\"].append(mtn)\n",
    "                                    d[\"SAFRAN\"].append(feature['title'])\n",
    "                                    d[\"ID\"].append(feature['station id'])\n",
    "                                    d[\"SAFRAN_S\"].append(feature['title_s'])\n",
    "                                    d[\"CLASS\"].append(pixel)\n",
    "                                    d[\"SD\"].append(sd)\n",
    "                                    count+=1\n",
    "                                    \n",
    "df = pd.DataFrame(data=d)   \n",
    "df = df.drop_duplicates()\n",
    "df[\"COLLECTION\"]=\"SWH\"\n",
    "df.to_pickle(df_swh_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f053d32-653c-4c1e-95f8-212d63a084d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "32a7d2fd-df59-40a8-a591-4247c918aed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYR 200030 30TXN 87%  3708                                               \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [43]\u001b[0m, in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m rasterio\u001b[38;5;241m.\u001b[39mopen(dlr) \u001b[38;5;28;01mas\u001b[39;00m src:\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature,list_coord,sd \u001b[38;5;129;01min\u001b[39;00m list_stations:\n\u001b[0;32m---> 58\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m z \u001b[38;5;129;01min\u001b[39;00m src\u001b[38;5;241m.\u001b[39msample(list_coord,masked\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m): \n\u001b[1;32m     59\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m z[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m : \n\u001b[1;32m     60\u001b[0m                 pixel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(z[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m/work/scratch/env/barrouz/conda_env/pl_lightning/lib/python3.8/site-packages/rasterio/sample.py:54\u001b[0m, in \u001b[0;36msample_gen\u001b[0;34m(dataset, xy, indexes, masked)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     window \u001b[38;5;241m=\u001b[39m Window(col_off, row_off, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmasked\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m data[:, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32mrasterio/_io.pyx:387\u001b[0m, in \u001b[0;36mrasterio._io.DatasetReaderBase.read\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/ma/core.py:6600\u001b[0m, in \u001b[0;36marray\u001b[0;34m(data, dtype, copy, order, mask, fill_value, keep_mask, hard_mask, shrink, subok, ndmin)\u001b[0m\n\u001b[1;32m   6596\u001b[0m masked \u001b[38;5;241m=\u001b[39m masked_singleton \u001b[38;5;241m=\u001b[39m MaskedConstant()\n\u001b[1;32m   6597\u001b[0m masked_array \u001b[38;5;241m=\u001b[39m MaskedArray\n\u001b[0;32m-> 6600\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21marray\u001b[39m(data, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   6601\u001b[0m           mask\u001b[38;5;241m=\u001b[39mnomask, fill_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keep_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   6602\u001b[0m           hard_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, shrink\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, subok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, ndmin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m   6603\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   6604\u001b[0m \u001b[38;5;124;03m    Shortcut to MaskedArray.\u001b[39;00m\n\u001b[1;32m   6605\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6608\u001b[0m \n\u001b[1;32m   6609\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   6610\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m MaskedArray(data, mask\u001b[38;5;241m=\u001b[39mmask, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[1;32m   6611\u001b[0m                        subok\u001b[38;5;241m=\u001b[39msubok, keep_mask\u001b[38;5;241m=\u001b[39mkeep_mask,\n\u001b[1;32m   6612\u001b[0m                        hard_mask\u001b[38;5;241m=\u001b[39mhard_mask, fill_value\u001b[38;5;241m=\u001b[39mfill_value,\n\u001b[1;32m   6613\u001b[0m                        ndmin\u001b[38;5;241m=\u001b[39mndmin, shrink\u001b[38;5;241m=\u001b[39mshrink, order\u001b[38;5;241m=\u001b[39morder)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#get days of landsat acqui per month/year/station...\n",
    "\n",
    "DLR_path = \"/home/ad/barrouz/zacharie/TIMESERIES_PROJECT/LANDSAT_QA_DLR\"\n",
    "df_dlr_path = \"/home/ad/barrouz/zacharie/TIMESERIES_PROJECT/INSITU/STATIONS/df_landsat_acquis_stations.pkl\"\n",
    "\n",
    "\n",
    "d = dict()\n",
    "d[\"ID\"] = []\n",
    "d[\"YEAR\"] = []\n",
    "d[\"MONTH\"] = []\n",
    "d[\"SEASON\"] = []\n",
    "d[\"DAY\"] = []\n",
    "d[\"MTN\"] = []\n",
    "d[\"TILE\"] = []\n",
    "d[\"SAFRAN\"] = []\n",
    "d[\"SAFRAN_S\"] = []\n",
    "d[\"CLASS\"] = []\n",
    "d[\"SD\"] = []\n",
    "nb_dates = len(list_dlr_dates)\n",
    "count = 0\n",
    "for mtn in S2_4326_tiles:\n",
    "    for wrs in LANDSAT_tiles[mtn]:\n",
    "        for j,single_date in enumerate(list_dlr_dates):\n",
    "            str_date = single_date.strftime(\"%Y%m%d\")\n",
    "            year = single_date.year\n",
    "            month = single_date.month\n",
    "            day = single_date.day\n",
    "            #get hydro date\n",
    "            if single_date < date(int(single_date.year),9,1):\n",
    "                annee_hydro = int(single_date.year) - 1\n",
    "            else:\n",
    "                annee_hydro = int(single_date.year)\n",
    "            date_debut_hydro = date(annee_hydro,9,1)\n",
    "            annee_hydro_jour = (single_date - date_debut_hydro).days + 1\n",
    "            #get list of dlr products\n",
    "            print(f\"{mtn} {wrs} {int(j/nb_dates*100)}%  {count}\",end=\"                                               \\r\")\n",
    "            list_dlr = glob.glob(os.path.join(DLR_path,mtn+\"_LIS\",f\"FSC_{str_date}T*_{wrs}\",f\"FSC_{str_date}T*_{wrs}_FSCTOC.tif\"),recursive=True)\n",
    "            if not list_dlr: continue\n",
    "            for k,dlr in enumerate(list_dlr):\n",
    "                for tile in LANDSAT_tiles[mtn][wrs]:\n",
    "                    #get list of stations with data at that date for that tile\n",
    "                    list_stations = []\n",
    "                    v = 0\n",
    "                    for feature,list_coord,station_df in dict_tile_stations[tile]:\n",
    "                        v+=1\n",
    "                        #print(f\"find stations {mtn} {tile} {day}/{month}/{year} {int(v/len(dict_tile_stations[tile])*100)}% {count}\",end=\"                                               \\r\")\n",
    "                        try:\n",
    "                            sd = station_df.loc[single_date.strftime(\"%Y-%m-%d\"), 'DSN_T_ISBA_Crocus_assim']\n",
    "                            list_stations.append((feature,list_coord,sd))\n",
    "                        except KeyError:\n",
    "                            continue\n",
    "                    if not list_stations : continue\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "            #check for swh/stations pairs\n",
    "            for k,dlr in enumerate(list_dlr):\n",
    "                #show progress\n",
    "                #print(f\"find pairs {mtn} {tile} {day}/{month}/{year} {int(k/len(list_swh)*100)}% {count}\",end=\"                                               \\r\")\n",
    "                with rasterio.open(dlr) as src:\n",
    "                    for feature,list_coord,sd in list_stations:\n",
    "                        for z in src.sample(list_coord,masked=True): \n",
    "                            if z[0] != '--' : \n",
    "                                pixel = float(z[0])\n",
    "                                #print(pixel)\n",
    "                                if pixel != 255 :\n",
    "                                    d[\"YEAR\"].append(annee_hydro)\n",
    "                                    d[\"DAY\"].append(annee_hydro_jour)\n",
    "                                    d[\"MONTH\"].append(month)\n",
    "                                    d[\"SEASON\"].append(season)\n",
    "                                    d[\"TILE\"].append(tile)\n",
    "                                    d[\"MTN\"].append(mtn)\n",
    "                                    d[\"SAFRAN\"].append(feature['title'])\n",
    "                                    d[\"ID\"].append(feature['station id'])\n",
    "                                    d[\"SAFRAN_S\"].append(feature['title_s'])\n",
    "                                    d[\"CLASS\"].append(pixel)\n",
    "                                    d[\"SD\"].append(sd)\n",
    "                                    count+=1\n",
    "                                    \n",
    "df = pd.DataFrame(data=d)   \n",
    "df = df.drop_duplicates()\n",
    "df[\"COLLECTION\"]=\"LANDSAT\"\n",
    "df.to_pickle(df_dlr_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96799ce1-c354-45e1-85a7-61f4235e82d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get days of landsat acqui per month/year/station...\n",
    "\n",
    "\n",
    "df_dlr_path = \"/home/ad/barrouz/zacharie/TIMESERIES_PROJECT/INSITU/STATIONS/df_landsat_acquis_stations.pkl\"\n",
    "dict_dlr_path = \"/home/ad/barrouz/zacharie/TIMESERIES_PROJECT/INSITU/STATIONS/dict_landsat_acquis_stations.json\"\n",
    "\n",
    "LANDSAT_path = \"/home/ad/barrouz/zacharie/TIMESERIES_PROJECT/LANDSAT_QA_DLR\"\n",
    "dict_stations = {}\n",
    "d = dict()\n",
    "d[\"ID\"] = []\n",
    "d[\"YEAR\"] = []\n",
    "d[\"MONTH\"] = []\n",
    "d[\"SEASON\"] = []\n",
    "d[\"DAY\"] = []\n",
    "d[\"MTN\"] = []\n",
    "d[\"TILE\"] = []\n",
    "d[\"SAFRAN\"] = []\n",
    "d[\"SAFRAN_S\"] = []\n",
    "d[\"CLASS\"] = []\n",
    "d[\"SD\"] = []\n",
    "count = 0\n",
    "len_points = len(layer_points)\n",
    "\n",
    "count_valid = 0\n",
    "for i,feature in enumerate(layer_points):\n",
    "    tile = feature['tile']\n",
    "    lon = feature['lon']\n",
    "    lat = feature['lat']\n",
    "    massif = feature['title']\n",
    "    massif_s = feature['title_s']\n",
    "    name = feature['station id']\n",
    "    df_station = get_station_data(name, min_year = 1985, max_year = 2015)\n",
    "    mtn = feature['mtn']\n",
    "    to_tile = pyproj.Transformer.from_crs(4326,int(epsg_list[tile]['EPSG']), always_xy=True)\n",
    "    coord = to_tile.itransform([(lon,lat)]) \n",
    "    list_coord = [*coord] #only one coord\n",
    "    dict_stations[name] = []\n",
    "    list_dates=[]\n",
    "    \n",
    "    \n",
    "    for wrs in LANDSAT_tiles_2[mtn][tile]:\n",
    "        for product in glob.glob(LANDSAT_path+\"/\"+mtn+\"_LIS/FSC_*_*_\"+wrs+\"/FSC_*_*_\"+wrs+\"_FSCTOC.tif\"):\n",
    "            dlr_date = getDateFromStr(product)\n",
    "            if dlr_date in list_dates : continue\n",
    "            if dlr_date.year < 1986 or dlr_date.year > 2015: continue\n",
    "            month = int(dlr_date.month)\n",
    "            day= int(dlr_date.day)\n",
    "            year= int(dlr_date.year)\n",
    "            if dlr_date < date(int(year),9,1):\n",
    "                annee_hydro = int(year) - 1\n",
    "            else:\n",
    "                annee_hydro = int(year)\n",
    "            date_debut_hydro = date(annee_hydro,9,1)\n",
    "            annee_hydro_jour = (dlr_date - date_debut_hydro).days + 1\n",
    "            season = \"AUTUMN\" if month in [9,10,11] else \"WINTER\" if month in [12,1,2] else \"SPRING\" if month in [3,4,5] else \"SUMMER\"\n",
    "            count = count +1\n",
    "            with rasterio.open(product) as src:\n",
    "                for z in src.sample(list_coord,masked=True): \n",
    "                    if z[0] != '--' : \n",
    "                        pixel = float(z[0])\n",
    " \n",
    "                        if pixel != 255 :\n",
    "                            count_valid = count_valid +1\n",
    "                            print(f\"{name} {year} {month} {day} {count_valid}\",end=\"                                               \\r\")\n",
    "                            sd = df_station.query(f\"wy == {annee_hydro} & dowy == {annee_hydro_jour}\").DSN_T_ISBA_Crocus_assim\n",
    "                            sd_value = np.nan\n",
    "                            if len(sd) == 1:\n",
    "                                sd_value = sd.item()\n",
    "                            d[\"YEAR\"].append(annee_hydro)\n",
    "                            d[\"DAY\"].append(annee_hydro_jour)\n",
    "                            d[\"MONTH\"].append(m)\n",
    "                            d[\"SEASON\"].append(season)\n",
    "                            d[\"TILE\"].append(tile)\n",
    "                            d[\"MTN\"].append(mtn)\n",
    "                            d[\"SAFRAN\"].append(massif)\n",
    "                            d[\"ID\"].append(name)\n",
    "                            d[\"SAFRAN_S\"].append(massif_s)\n",
    "                            d[\"CLASS\"].append(pixel)\n",
    "                            d[\"SD\"].append(sd_value)\n",
    "                            dict_stations[name].append(product)\n",
    "                            list_dates.append(dlr_date)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "with open(dict_dlr_path, \"w\") as outfile: \n",
    "    json.dump(dict_stations, outfile)\n",
    "    \n",
    "df = pd.DataFrame(data=d)   \n",
    "df = df.drop_duplicates()\n",
    "df[\"COLLECTION\"]=\"LANDSAT\"\n",
    "df.to_pickle(df_dlr_path)\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa9c1db-3c0a-45bc-81b7-a44bdba651f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_swh_path = \"/home/ad/barrouz/zacharie/TIMESERIES_PROJECT/INSITU/STATIONS/df_swh_acquis_stations.pkl\"\n",
    "df_dlr_path = \"/home/ad/barrouz/zacharie/TIMESERIES_PROJECT/INSITU/STATIONS/df_landsat_acquis_stations.pkl\"\n",
    "with open( df_swh_path, \"rb\" )  as f1, open( df_dlr_path, \"rb\" )  as f2:\n",
    "    df = pd.concat([pickle.load(f1),pickle.load(f2)],ignore_index=True)\n",
    "df[\"MONTH\"] = df['MONTH'].apply(lambda m: calendar.month_abbr[m])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde16fed-0a15-4e9a-832f-3b3b420e9d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_path = \"/home/ad/barrouz/zacharie/TIMESERIES_PROJECT/SYNTHESIS/ANALYSIS/SAFRAN/PLOTS\"\n",
    "df_all = df.drop(columns=['COLLECTION']).drop_duplicates()\n",
    "month_hydro_order = calendar.month_abbr[9:]+calendar.month_abbr[1:9]\n",
    "print(month_hydro_order)\n",
    "\n",
    "\n",
    "df_temp1 = df_all.groupby([\"MTN\",\"MONTH\",\"ID\",\"YEAR\"])[\"DAY\"].count().to_frame(\"MEAN ACQUISITION COUNT\").reset_index()\n",
    "df_temp2 = df_all.groupby([\"MTN\",\"ID\",\"YEAR\"])[\"DAY\"].count().to_frame(\"MEAN ACQUISITION COUNT\").reset_index()\n",
    "print(df_temp1)\n",
    "print(df_temp2)\n",
    "#df_temp2 = df_all.groupby([\"MTN\",\"MONTH\",\"ID\",\"YEAR\"])[\"DAY\"].count().to_frame(\"ACQUISITION COUNT\").reset_index()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2)\n",
    "sn.barplot(ax=axs[0],data=df_temp1.set_index(\"MONTH\").loc[month_hydro_order],x='MONTH',y='MEAN ACQUISITION COUNT',hue='MTN',legend=False)\n",
    "axs[0].set_ylabel(\"MEAN ACQUISITION COUNT\",size=20)\n",
    "axs[0].set_xlabel(\"MONTH\",size=20)\n",
    "axs[0].tick_params(axis='both', which='major', labelsize=18)\n",
    "sn.barplot(ax=axs[1],data=df_temp2,x='YEAR',y='MEAN ACQUISITION COUNT',hue='MTN',legend=False)\n",
    "axs[1].set_ylabel(\"MEAN ACQUISITION COUNT\",size=20)\n",
    "axs[1].set_xlabel(\"HYDROLOGICAL YEAR\",size=20)\n",
    "axs[1].tick_params(axis='both', which='major', labelsize=18)\n",
    "plt.xticks(rotation = 45)\n",
    "fig.set_figwidth(22)\n",
    "fig.set_figheight(16)\n",
    "plt.savefig(op.join(plot_path,f'stations_acqui.svg'),format=\"svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48020616-79cf-4e93-91d2-f7947a13cba9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_path = \"/home/ad/barrouz/zacharie/TIMESERIES_PROJECT/SYNTHESIS/ANALYSIS/S2_BIAS/DATAFRAMES/df_swh_landsat_acquis.pkl\"\n",
    "with open( df_path, \"rb\" )  as df:\n",
    "    df = pickle.load(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f1786f-5f70-42bf-80e4-68b00e88244f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make monthly distribution of nb of acquisitions \n",
    "df_path = \"/home/ad/barrouz/zacharie/TIMESERIES_PROJECT/SYNTHESIS/ANALYSIS/S2_BIAS/DATAFRAMES/df_swh_landsat_acquis.pkl\"\n",
    "with open( df_path, \"rb\" )  as df:\n",
    "    df = pickle.load(df)\n",
    "\n",
    "df_temp = df.query(\"YEAR >= 1986 & YEAR < 2015\")\n",
    "df_temp = df_temp[[\"MONTH\",\"MTN\",\"ID\"]].groupby([])\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "df[\"ACQUI\"]  =1\n",
    "df_minmax = df[[\"YEAR\",\"ID\",\"ACQUI\"]].groupby([\"YEAR\",\"ID\"], as_index=False).sum()\n",
    "\n",
    "\n",
    "min_obs = df_minmax[\"ACQUI\"].min()\n",
    "max_obs = df_minmax[\"ACQUI\"].max()\n",
    "print(min_obs,max_obs)\n",
    "\n",
    "df_seasons = df[[\"SEASON\",\"ID\",\"ACQUI\"]].groupby([\"SEASON\",\"ID\"], as_index=False).sum()\n",
    "fig, ax = plt.subplots()\n",
    "sn.boxplot(data = df_seasons, x = \"SEASON\",y=\"ACQUI\",order=[\"AUTUMN\",\"WINTER\",\"SPRING\",\"SUMMER\"])\n",
    "plt.savefig(\"/home/ad/barrouz/zacharie/TIMESERIES_PROJECT/SYNTHESIS/ANALYSIS/S2_BIAS/PLOTS/seasons.pdf\",format=\"pdf\")\n",
    "\n",
    "df = df_seasons[[\"SEASON\",\"ACQUI\"]].groupby([\"SEASON\"], as_index=False).median()\n",
    "df[\"WEIGHT\"] = df[\"ACQUI\"]/df[\"ACQUI\"].sum()\n",
    "dict_weights = dict(zip(df[\"SEASON\"], df[\"WEIGHT\"]))\n",
    "        \n",
    "print(dict_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6982bb31-e113-4554-9012-7fa8cbdbb926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make seasonal weigth range and get min and max NOBS\n",
    "df_path = \"/home/ad/barrouz/zacharie/TIMESERIES_PROJECT/SYNTHESIS/ANALYSIS/S2_BIAS/DATAFRAMES/df_swh_landsat_acquis.pkl\"\n",
    "with open( df_path, \"rb\" )  as df:\n",
    "    df = pickle.load(df)\n",
    "\n",
    "df = df.query(\"YEAR >= 1986 & YEAR < 2015\")\n",
    "df = df[[\"YEAR\",\"SEASON\",\"DAY\",\"ID\"]]\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "df[\"ACQUI\"]  =1\n",
    "df_minmax = df[[\"YEAR\",\"ID\",\"ACQUI\"]].groupby([\"YEAR\",\"ID\"], as_index=False).sum()\n",
    "\n",
    "\n",
    "min_obs = df_minmax[\"ACQUI\"].min()\n",
    "max_obs = df_minmax[\"ACQUI\"].max()\n",
    "print(min_obs,max_obs)\n",
    "\n",
    "df_seasons = df[[\"SEASON\",\"ID\",\"ACQUI\"]].groupby([\"SEASON\",\"ID\"], as_index=False).sum()\n",
    "fig, ax = plt.subplots()\n",
    "sn.boxplot(data = df_seasons, x = \"SEASON\",y=\"ACQUI\",order=[\"AUTUMN\",\"WINTER\",\"SPRING\",\"SUMMER\"])\n",
    "plt.savefig(\"/home/ad/barrouz/zacharie/TIMESERIES_PROJECT/SYNTHESIS/ANALYSIS/S2_BIAS/PLOTS/seasons.pdf\",format=\"pdf\")\n",
    "\n",
    "df = df_seasons[[\"SEASON\",\"ACQUI\"]].groupby([\"SEASON\"], as_index=False).median()\n",
    "df[\"WEIGHT\"] = df[\"ACQUI\"]/df[\"ACQUI\"].sum()\n",
    "dict_weights = dict(zip(df[\"SEASON\"], df[\"WEIGHT\"]))\n",
    "        \n",
    "print(dict_weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pl_lightning",
   "language": "python",
   "name": "pl_lightning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
